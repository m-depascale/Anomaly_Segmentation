{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**BiSeNet Training**"
      ],
      "metadata": {
        "id": "iPW8i9duaujb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7WBJQxqaphO"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/m-depascale/AnomalySegmentation_AML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ood-metrics\n",
        "!pip install visdom\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install Pillow\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "9NAGfoqQbGNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install cityscapesscripts\n",
        "!python -m pip install cityscapesscripts[gui]"
      ],
      "metadata": {
        "id": "17glmzuTbQV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cityscapesscripts.download import downloader\n",
        "print(dir(downloader))\n",
        "import os\n",
        "\n",
        "# Esegui il login\n",
        "session = downloader.login()\n",
        "names = downloader.list_available_packages(session=session)\n",
        "print(names)\n",
        "\n",
        "\n",
        "destination_path = \"/content/Datasets/Cityscapes\"\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "downloader.download_packages(session=session, package_names=['leftImg8bit_trainvaltest.zip', 'gtFine_trainvaltest.zip'], destination_path=destination_path)"
      ],
      "metadata": {
        "id": "iYOkRw2abVvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Datasets/Cityscapes/leftImg8bit_trainvaltest.zip -d /content/Datasets/Cityscapes/"
      ],
      "metadata": {
        "id": "W_3_bTwXcZOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Datasets/Cityscapes/gtFine_trainvaltest.zip -d /content/Datasets/Cityscapes/"
      ],
      "metadata": {
        "id": "zPrp92ZTcdjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CITYSCAPES_DATASET'] = '/content/Datasets/Cityscapes'\n",
        "from cityscapesscripts.preparation.createTrainIdLabelImgs import main as convertor\n",
        "convertor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nSNkA08cow1",
        "outputId": "d29a8cdd-aa6c-40d8-c416-b9b4218794d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "sqLH5n-9k8ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!gdown 1r2eFANvSlcUjxcerjC8l6dRa0slowMpx"
      ],
      "metadata": {
        "id": "OIOZOdvlk_84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Validation_Dataset.zip -d /content"
      ],
      "metadata": {
        "id": "LHM0__KTlFL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/AnomalySegmentation_AML/eval/bisenetv1.py /content/AnomalySegmentation_AML/train/bisenetv1.py"
      ],
      "metadata": {
        "id": "UUmbWQlldWOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/AnomalySegmentation_AML/eval/resnet.py /content/AnomalySegmentation_AML/train/resnet.py"
      ],
      "metadata": {
        "id": "BJJlH_gCdhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AnomalySegmentation_AML/trained_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNv28ctKd0ti",
        "outputId": "d3750212-3114-43dd-8910-348d7d4e4b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AnomalySegmentation_AML/trained_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first part\n",
        "!unrar x /content/AnomalySegmentation_AML/trained_models/bisenet_finetuned.part1.rar\n",
        "\n",
        "# Extract the second part\n",
        "!unrar x /content/AnomalySegmentation_AML/trained_models/bisenet_finetuned.part2.rar\n",
        "\n",
        "#Select \"All\" when colab asks you something.\n",
        "#Then you will find bisenet_finetuned.pth on /content/bisenet_finetuned.pth"
      ],
      "metadata": {
        "id": "aYIkyzoid6d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AnomalySegmentation_AML/eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5AeLAGbdtAh",
        "outputId": "1422b3b0-2b64-4041-b465-ba890aa2bd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AnomalySegmentation_AML/eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_iou.py --datadir \"/content/Datasets/Cityscapes\" --subset 'val' --loadWeights='bisenet_finetuned.pth' --loadModel='bisenetv1.py'"
      ],
      "metadata": {
        "id": "-0pWEUR2dwWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python evalAnomaly.py --method \"VOID\" --loadModel='bisenetv1.py' --loadWeights='bisenet_finetuned.pth' --input \"/content/Validation_Dataset/RoadObsticle21/images/*.webp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKJenpqdeQ1a",
        "outputId": "f743034e-50a0-4c9d-ef85-668318f22272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: ../trained_models/bisenetv1.py\n",
            "Loading weights: ../trained_models/bisenet_finetuned.pth\n",
            "--- SAVED PRETRAINED PARAMS ---- \n",
            "Model and weights LOADED successfully\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "AUPRC score: 0.6374021338018769\n",
            "FPR@TPR95: 98.7324336447037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evalAnomaly.py --method \"VOID\" --loadModel='bisenetv1.py' --loadWeights='bisenet_finetuned.pth' --input \"/content/Validation_Dataset/RoadAnomaly21/images/*.png\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm4wX5UdmnHI",
        "outputId": "6ea78b3c-31c1-478b-9b52-c915dc6a553b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: ../trained_models/bisenetv1.py\n",
            "Loading weights: ../trained_models/bisenet_finetuned.pth\n",
            "--- SAVED PRETRAINED PARAMS ---- \n",
            "Model and weights LOADED successfully\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "AUPRC score: 19.868139792749435\n",
            "FPR@TPR95: 94.76721506874186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evalAnomaly.py --method \"VOID\" --loadModel='bisenetv1.py' --loadWeights='bisenet_finetuned.pth' --input \"/content/Validation_Dataset/RoadAnomaly/images/*.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZPZt9VemoHf",
        "outputId": "3c791e19-6d0c-43f6-d501-87f4df632fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/AnomalySegmentation_AML/train/evalAnomaly.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evalAnomaly.py --method \"VOID\" --loadModel='bisenetv1.py' --loadWeights='bisenet_finetuned.pth' --input \"/content/Validation_Dataset/fs_static/images/*.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZusXXGRSmz7h",
        "outputId": "26dea2bd-efcd-4839-89c0-2d4c8919b3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: ../trained_models/bisenetv1.py\n",
            "Loading weights: ../trained_models/bisenet_finetuned.pth\n",
            "--- SAVED PRETRAINED PARAMS ---- \n",
            "Model and weights LOADED successfully\n",
            "AUPRC score: 1.5074097402817843\n",
            "FPR@TPR95: 97.86444374061732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evalAnomaly.py --method \"VOID\" --loadModel='bisenetv1.py' --loadWeights='bisenet_finetuned.pth' --input \"/content/Validation_Dataset/FS_LostFound_full/images/*.png\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckd4dG95m4H5",
        "outputId": "8c134bde-d647-4853-dee2-83c4806d9739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: ../trained_models/bisenetv1.py\n",
            "Loading weights: ../trained_models/bisenet_finetuned.pth\n",
            "--- SAVED PRETRAINED PARAMS ---- \n",
            "Model and weights LOADED successfully\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AnomalySegmentation_AML/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvQcMZwxm4BI",
        "outputId": "8c0870a3-205c-483d-bfa5-d9c8a956df5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AnomalySegmentation_AML/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.optim import SGD, Adam, lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize, Pad\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import VOC12,cityscapes\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "from visualize import Dashboard\n",
        "\n",
        "import importlib\n",
        "from iouEval import iouEval, getColorEntry\n",
        "\n",
        "from shutil import copyfile\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20 #pascal=22, cityscapes=20\n",
        "\n",
        "color_transform = Colorize(NUM_CLASSES)\n",
        "image_transform = ToPILImage()\n",
        "\n",
        "#Augmentations - different function implemented to perform random augments on both image and target\n",
        "class MyCoTransform(object):\n",
        "    def __init__(self, enc, augment=True, height=512):\n",
        "        self.enc=enc\n",
        "        self.augment = augment\n",
        "        self.height = height\n",
        "        pass\n",
        "    def __call__(self, input, target):\n",
        "        # do something to both images\n",
        "        input =  Resize(self.height, Image.BILINEAR)(input)\n",
        "        target = Resize(self.height, Image.NEAREST)(target)\n",
        "\n",
        "        if(self.augment):\n",
        "            # Random hflip\n",
        "            hflip = random.random()\n",
        "            if (hflip < 0.5):\n",
        "                input = input.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                target = target.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            #Random translation 0-2 pixels (fill rest with padding\n",
        "            transX = random.randint(-2, 2)\n",
        "            transY = random.randint(-2, 2)\n",
        "\n",
        "            input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)\n",
        "            target = ImageOps.expand(target, border=(transX,transY,0,0), fill=255) #pad label filling with 255\n",
        "            input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))\n",
        "            target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))\n",
        "\n",
        "        input = ToTensor()(input)\n",
        "        if (self.enc):\n",
        "            target = Resize(int(self.height/8), Image.NEAREST)(target)\n",
        "        target = ToLabel()(target)\n",
        "        target = Relabel(255, 19)(target)\n",
        "\n",
        "        return input, target\n",
        "\n",
        "\n",
        "class CrossEntropyLoss2d(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss = torch.nn.NLLLoss2d(weight)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        return self.loss(torch.nn.functional.log_softmax(outputs, dim=1), targets)\n",
        "\n",
        "\n",
        "def train(args, model, enc=False):\n",
        "    best_acc = 0\n",
        "\n",
        "    #TODO: calculate weights by processing dataset histogram (now its being set by hand from the torch values)\n",
        "    #create a loder to run all images and calculate histogram of labels, then create weight array using class balancing\n",
        "\n",
        "    weight = torch.ones(NUM_CLASSES)\n",
        "    if (enc):\n",
        "        weight[0] = 2.3653597831726\n",
        "        weight[1] = 4.4237880706787\n",
        "        weight[2] = 2.9691488742828\n",
        "        weight[3] = 5.3442072868347\n",
        "        weight[4] = 5.2983593940735\n",
        "        weight[5] = 5.2275490760803\n",
        "        weight[6] = 5.4394111633301\n",
        "        weight[7] = 5.3659925460815\n",
        "        weight[8] = 3.4170460700989\n",
        "        weight[9] = 5.2414722442627\n",
        "        weight[10] = 4.7376127243042\n",
        "        weight[11] = 5.2286224365234\n",
        "        weight[12] = 5.455126285553\n",
        "        weight[13] = 4.3019247055054\n",
        "        weight[14] = 5.4264230728149\n",
        "        weight[15] = 5.4331531524658\n",
        "        weight[16] = 5.433765411377\n",
        "        weight[17] = 5.4631009101868\n",
        "        weight[18] = 5.3947434425354\n",
        "    else:\n",
        "        weight[0] = 2.8149201869965\n",
        "        weight[1] = 6.9850029945374\n",
        "        weight[2] = 3.7890393733978\n",
        "        weight[3] = 9.9428062438965\n",
        "        weight[4] = 9.7702074050903\n",
        "        weight[5] = 9.5110931396484\n",
        "        weight[6] = 10.311357498169\n",
        "        weight[7] = 10.026463508606\n",
        "        weight[8] = 4.6323022842407\n",
        "        weight[9] = 9.5608062744141\n",
        "        weight[10] = 7.8698215484619\n",
        "        weight[11] = 9.5168733596802\n",
        "        weight[12] = 10.373730659485\n",
        "        weight[13] = 6.6616044044495\n",
        "        weight[14] = 10.260489463806\n",
        "        weight[15] = 10.287888526917\n",
        "        weight[16] = 10.289801597595\n",
        "        weight[17] = 10.405355453491\n",
        "        weight[18] = 10.138095855713\n",
        "\n",
        "    weight[19] = 0\n",
        "\n",
        "    assert os.path.exists(args['datadir']), \"Error: datadir (dataset directory) could not be loaded\"\n",
        "\n",
        "    co_transform = MyCoTransform(enc, augment=True, height=args['height'])#1024)\n",
        "    co_transform_val = MyCoTransform(enc, augment=False, height=args['height'])#1024)\n",
        "    dataset_train = cityscapes(args['datadir'], co_transform, 'train')\n",
        "    dataset_val = cityscapes(args['datadir'], co_transform_val, 'val')\n",
        "\n",
        "    loader = DataLoader(dataset_train, num_workers=args['num_workers'], batch_size=args['batch_size'], shuffle=True)\n",
        "    loader_val = DataLoader(dataset_val, num_workers=args['num_workers'], batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "    if args['cuda']:\n",
        "        weight = weight.cuda()\n",
        "    criterion = CrossEntropyLoss2d(weight)\n",
        "    print(type(criterion))\n",
        "\n",
        "    savedir = args['savedir']\n",
        "\n",
        "    if (enc):\n",
        "        automated_log_path = savedir + \"/automated_log_encoder.txt\"\n",
        "        modeltxtpath = savedir + \"/model_encoder.txt\"\n",
        "    else:\n",
        "        automated_log_path = savedir + \"/automated_log.txt\"\n",
        "        modeltxtpath = savedir + \"/model.txt\"\n",
        "\n",
        "    if (not os.path.exists(automated_log_path)):    #dont add first line if it exists\n",
        "        with open(automated_log_path, \"a\") as myfile:\n",
        "            myfile.write(\"Epoch\\t\\tTrain-loss\\t\\tTest-loss\\t\\tTrain-IoU\\t\\tTest-IoU\\t\\tlearningRate\")\n",
        "\n",
        "    with open(modeltxtpath, \"w\") as myfile:\n",
        "        myfile.write(str(model))\n",
        "\n",
        "\n",
        "    #optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=2e-4)     ## scheduler 1\n",
        "    optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4)      ## scheduler 2\n",
        "\n",
        "    start_epoch = 1\n",
        "    if args['resume']:\n",
        "        #Must load weights, optimizer, epoch and best value.\n",
        "        if enc:\n",
        "            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\n",
        "        else:\n",
        "            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\n",
        "\n",
        "        assert os.path.exists(filenameCheckpoint), \"Error: resume option was used but checkpoint was not found in folder\"\n",
        "        checkpoint = torch.load(filenameCheckpoint)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        print(\"=> Loaded checkpoint at epoch {})\".format(checkpoint['epoch']))\n",
        "\n",
        "    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5) # set up scheduler     ## scheduler 1\n",
        "    lambda1 = lambda epoch: pow((1-((epoch-1)/args['num_epochs'])),0.9)  ## scheduler 2\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)                             ## scheduler 2\n",
        "\n",
        "    if args['visualize'] and args['steps_plot'] > 0:\n",
        "        board = Dashboard(8097)\n",
        "\n",
        "    for epoch in range(start_epoch, args['num_epochs']+1):\n",
        "        print(\"----- TRAINING - EPOCH\", epoch, \"-----\")\n",
        "        #print(weight)\n",
        "        scheduler.step(epoch)    ## scheduler 2\n",
        "\n",
        "        epoch_loss = []\n",
        "        time_train = []\n",
        "\n",
        "        doIouTrain = args['iouTrain']\n",
        "        doIouVal =  args['iouVal']\n",
        "\n",
        "        if (doIouTrain):\n",
        "            iouEvalTrain = iouEval(NUM_CLASSES)\n",
        "\n",
        "        usedLr = 0\n",
        "        for param_group in optimizer.param_groups:\n",
        "            print(\"LEARNING RATE: \", param_group['lr'])\n",
        "            usedLr = float(param_group['lr'])\n",
        "\n",
        "        model.train()\n",
        "        for step, (images, labels) in enumerate(loader):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            if args['cuda']:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            inputs = Variable(images)\n",
        "            targets = Variable(labels)\n",
        "            outputs, *_ = model(inputs)\n",
        "            #print(outputs.shape)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(outputs, targets[:, 0])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss.append(loss.data.item())\n",
        "            time_train.append(time.time() - start_time)\n",
        "\n",
        "            if (doIouTrain):\n",
        "                iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
        "\n",
        "            #print(outputs.size())\n",
        "            if args['visualize'] and args['steps_plot'] > 0 and step % args['steps_plot'] == 0:\n",
        "                start_time_plot = time.time()\n",
        "                image = inputs[0].cpu().data\n",
        "\n",
        "                board.image(image, f'input (epoch: {epoch}, step: {step})')\n",
        "                if isinstance(outputs, list):   #merge gpu tensors\n",
        "                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'output (epoch: {epoch}, step: {step})')\n",
        "                else:\n",
        "                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'output (epoch: {epoch}, step: {step})')\n",
        "                board.image(color_transform(targets[0].cpu().data),\n",
        "                    f'target (epoch: {epoch}, step: {step})')\n",
        "                print (\"Time to paint images: \", time.time() - start_time_plot)\n",
        "            if args['steps_loss'] > 0 and step % args['steps_loss'] == 0:\n",
        "                average = sum(epoch_loss) / len(epoch_loss)\n",
        "                print(f'loss: {average:0.4} (epoch: {epoch}, step: {step})',\n",
        "                        \"// Avg time/img: %.4f s\" % (sum(time_train) / len(time_train) / args['batch_size']))\n",
        "\n",
        "\n",
        "        average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n",
        "\n",
        "        iouTrain = 0\n",
        "        if (doIouTrain):\n",
        "            iouTrain, iou_classes = iouEvalTrain.getIoU()\n",
        "            iouStr = getColorEntry(iouTrain)+'{:0.2f}'.format(iouTrain*100) + '\\033[0m'\n",
        "            print (\"EPOCH IoU on TRAIN set: \", iouStr, \"%\")\n",
        "\n",
        "        #Validate on 500 val images after each epoch of training\n",
        "        print(\"----- VALIDATING - EPOCH\", epoch, \"-----\")\n",
        "        model.eval()\n",
        "        epoch_loss_val = []\n",
        "        time_val = []\n",
        "\n",
        "        if (doIouVal):\n",
        "            iouEvalVal = iouEval(NUM_CLASSES)\n",
        "\n",
        "        for step, (images, labels) in enumerate(loader_val):\n",
        "            start_time = time.time()\n",
        "            if args['cuda']:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
        "            targets = Variable(labels, volatile=True)\n",
        "            outputs, *_ = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets[:, 0])\n",
        "            epoch_loss_val.append(loss.data.item())\n",
        "            time_val.append(time.time() - start_time)\n",
        "\n",
        "\n",
        "            #Add batch to calculate TP, FP and FN for iou estimation\n",
        "            if (doIouVal):\n",
        "                iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
        "\n",
        "            if args['visualize'] and args['steps_plot'] > 0 and step % args['steps_plot'] == 0:\n",
        "                start_time_plot = time.time()\n",
        "                image = inputs[0].cpu().data\n",
        "                board.image(image, f'VAL input (epoch: {epoch}, step: {step})')\n",
        "                if isinstance(outputs, list):   #merge gpu tensors\n",
        "                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'VAL output (epoch: {epoch}, step: {step})')\n",
        "                else:\n",
        "                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'VAL output (epoch: {epoch}, step: {step})')\n",
        "                board.image(color_transform(targets[0].cpu().data),\n",
        "                    f'VAL target (epoch: {epoch}, step: {step})')\n",
        "                print (\"Time to paint images: \", time.time() - start_time_plot)\n",
        "            if args['steps_loss'] > 0 and step % args['steps_loss'] == 0:\n",
        "                average = sum(epoch_loss_val) / len(epoch_loss_val)\n",
        "                print(f'VAL loss: {average:0.4} (epoch: {epoch}, step: {step})',\n",
        "                        \"// Avg time/img: %.4f s\" % (sum(time_val) / len(time_val) / args['batch_size']))\n",
        "\n",
        "\n",
        "        average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)\n",
        "\n",
        "        iouVal = 0\n",
        "        if (doIouVal):\n",
        "            iouVal, iou_classes = iouEvalVal.getIoU()\n",
        "            iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\n",
        "            print (\"EPOCH IoU on VAL set: \", iouStr, \"%\")\n",
        "\n",
        "\n",
        "        if iouVal == 0:\n",
        "            current_acc = -average_epoch_loss_val\n",
        "        else:\n",
        "            current_acc = iouVal\n",
        "        is_best = current_acc > best_acc\n",
        "        best_acc = max(current_acc, best_acc)\n",
        "        if enc:\n",
        "            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\n",
        "            filenameBest = savedir + '/model_best_enc.pth.tar'\n",
        "        else:\n",
        "            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\n",
        "            filenameBest = savedir + '/model_best.pth.tar'\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'arch': str(model),\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_acc': best_acc,\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "        }, is_best, filenameCheckpoint, filenameBest)\n",
        "\n",
        "        #SAVE MODEL AFTER EPOCH\n",
        "        if (enc):\n",
        "            filename = f'{savedir}/model_encoder-{epoch:03}.pth'\n",
        "            filenamebest = f'{savedir}/model_encoder_best.pth'\n",
        "        else:\n",
        "            filename = f'{savedir}/model-{epoch:03}.pth'\n",
        "            filenamebest = f'{savedir}/model_best.pth'\n",
        "        if args['epochs_save'] > 0 and step > 0 and step % args['epochs_save'] == 0:\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            print(f'save: {filename} (epoch: {epoch})')\n",
        "        if (is_best):\n",
        "            torch.save(model.state_dict(), filenamebest)\n",
        "            print(f'save: {filenamebest} (epoch: {epoch})')\n",
        "            if (not enc):\n",
        "                with open(savedir + \"/best.txt\", \"w\") as myfile:\n",
        "                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))\n",
        "            else:\n",
        "                with open(savedir + \"/best_encoder.txt\", \"w\") as myfile:\n",
        "                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))\n",
        "\n",
        "        #SAVE TO FILE A ROW WITH THE EPOCH RESULT (train loss, val loss, train IoU, val IoU)\n",
        "        #Epoch\t\tTrain-loss\t\tTest-loss\tTrain-IoU\tTest-IoU\t\tlearningRate\n",
        "        with open(automated_log_path, \"a\") as myfile:\n",
        "            myfile.write(\"\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.8f\" % (epoch, average_epoch_loss_train, average_epoch_loss_val, iouTrain, iouVal, usedLr ))\n",
        "\n",
        "    return(model)   #return model (convenience for encoder-decoder training)\n",
        "\n",
        "def save_checkpoint(state, is_best, filenameCheckpoint, filenameBest):\n",
        "    torch.save(state, filenameCheckpoint)\n",
        "    if is_best:\n",
        "        print (\"Saving model as best\")\n",
        "        torch.save(state, filenameBest)"
      ],
      "metadata": {
        "id": "9jqkXfiQnVgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bisenetv1 import BiSeNetV1\n",
        "\n",
        "model = BiSeNetV1(NUM_CLASSES)"
      ],
      "metadata": {
        "id": "ivFgEvRbnb0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import logging.handlers\n",
        "import sys\n",
        "\n",
        "\n",
        "savedir = f'../save/BiSeNet_finetuned'\n",
        "if not os.path.exists(savedir):\n",
        "        os.makedirs(savedir)\n",
        "\n",
        "args = {\n",
        "    'height': 512,\n",
        "    'num_workers': 2,\n",
        "    'batch_size' : 8,\n",
        "    'cuda' : True,\n",
        "    'savedir' : savedir,\n",
        "    'resume': False,\n",
        "    'num_epochs': 20,\n",
        "    'steps_plot' : 50,\n",
        "    'visualize' : False,\n",
        "    'iouTrain': False,\n",
        "    'iouVal': True,\n",
        "    'steps_loss' : 50,\n",
        "    'epochs_save': 0,\n",
        "    'datadir': f'/content/Datasets/Cityscapes/',\n",
        "    'resume': False\n",
        "}\n",
        "\n",
        "\n",
        "bisenet_model = BiSeNetV1(NUM_CLASSES)\n",
        "\n",
        "keys = ['conv_out.conv_out',\n",
        "        'conv_out16.conv_out',\n",
        "        'conv_out32.conv_out']\n",
        "\n",
        "new_dict = torch.load('/content/AnomalySegmentation_AML/trained_models/bisenet_finetuned.pth')\n",
        "\n",
        "model.load_state_dict(new_dict)\n",
        "model.cuda()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  splits = name.split('.')\n",
        "  if str(splits[0]+'.'+splits[1]) in keys:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "bisenet_fine_tuned = train(args, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q5PSCJWneXY",
        "outputId": "e505c81e-ed64-4bf7-a677-f962b4a6909e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Datasets/Cityscapes/leftImg8bit/train\n",
            "/content/Datasets/Cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:222: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n",
            "  warnings.warn(\"NLLLoss2d has been deprecated. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.1899 (epoch: 1, step: 0) // Avg time/img: 0.0430 s\n",
            "loss: 0.2393 (epoch: 1, step: 50) // Avg time/img: 0.0212 s\n",
            "loss: 0.2666 (epoch: 1, step: 100) // Avg time/img: 0.0211 s\n",
            "loss: 0.2939 (epoch: 1, step: 150) // Avg time/img: 0.0212 s\n",
            "loss: 0.303 (epoch: 1, step: 200) // Avg time/img: 0.0211 s\n",
            "loss: 0.2985 (epoch: 1, step: 250) // Avg time/img: 0.0211 s\n",
            "loss: 0.2991 (epoch: 1, step: 300) // Avg time/img: 0.0212 s\n",
            "loss: 0.2966 (epoch: 1, step: 350) // Avg time/img: 0.0212 s\n",
            "----- VALIDATING - EPOCH 1 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-1e75ccddfce5>:277: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "<ipython-input-95-1e75ccddfce5>:278: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL loss: 0.2714 (epoch: 1, step: 0) // Avg time/img: 0.0123 s\n",
            "VAL loss: 0.4081 (epoch: 1, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.88\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.00047744269081074987\n",
            "loss: 0.2519 (epoch: 2, step: 0) // Avg time/img: 0.0244 s\n",
            "loss: 0.299 (epoch: 2, step: 50) // Avg time/img: 0.0211 s\n",
            "loss: 0.2837 (epoch: 2, step: 100) // Avg time/img: 0.0212 s\n",
            "loss: 0.286 (epoch: 2, step: 150) // Avg time/img: 0.0212 s\n",
            "loss: 0.2812 (epoch: 2, step: 200) // Avg time/img: 0.0212 s\n",
            "loss: 0.2777 (epoch: 2, step: 250) // Avg time/img: 0.0212 s\n",
            "loss: 0.2824 (epoch: 2, step: 300) // Avg time/img: 0.0212 s\n",
            "loss: 0.278 (epoch: 2, step: 350) // Avg time/img: 0.0212 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.226 (epoch: 2, step: 0) // Avg time/img: 0.0132 s\n",
            "VAL loss: 0.3522 (epoch: 2, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m57.38\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.00045476628804148113\n",
            "loss: 0.2136 (epoch: 3, step: 0) // Avg time/img: 0.0250 s\n",
            "loss: 0.2596 (epoch: 3, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.2472 (epoch: 3, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.2434 (epoch: 3, step: 150) // Avg time/img: 0.0215 s\n",
            "loss: 0.2458 (epoch: 3, step: 200) // Avg time/img: 0.0214 s\n",
            "loss: 0.2471 (epoch: 3, step: 250) // Avg time/img: 0.0214 s\n",
            "loss: 0.2477 (epoch: 3, step: 300) // Avg time/img: 0.0214 s\n",
            "loss: 0.2483 (epoch: 3, step: 350) // Avg time/img: 0.0213 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.2612 (epoch: 3, step: 0) // Avg time/img: 0.0142 s\n",
            "VAL loss: 0.415 (epoch: 3, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m54.93\u001b[0m %\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.00043196348615140955\n",
            "loss: 0.2854 (epoch: 4, step: 0) // Avg time/img: 0.0256 s\n",
            "loss: 0.273 (epoch: 4, step: 50) // Avg time/img: 0.0213 s\n",
            "loss: 0.263 (epoch: 4, step: 100) // Avg time/img: 0.0213 s\n",
            "loss: 0.2565 (epoch: 4, step: 150) // Avg time/img: 0.0212 s\n",
            "loss: 0.2529 (epoch: 4, step: 200) // Avg time/img: 0.0213 s\n",
            "loss: 0.2514 (epoch: 4, step: 250) // Avg time/img: 0.0212 s\n",
            "loss: 0.2529 (epoch: 4, step: 300) // Avg time/img: 0.0212 s\n",
            "loss: 0.2533 (epoch: 4, step: 350) // Avg time/img: 0.0213 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.223 (epoch: 4, step: 0) // Avg time/img: 0.0117 s\n",
            "VAL loss: 0.3547 (epoch: 4, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m59.08\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.00040902607302542923\n",
            "loss: 0.2535 (epoch: 5, step: 0) // Avg time/img: 0.0253 s\n",
            "loss: 0.2253 (epoch: 5, step: 50) // Avg time/img: 0.0221 s\n",
            "loss: 0.2242 (epoch: 5, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.2248 (epoch: 5, step: 150) // Avg time/img: 0.0217 s\n",
            "loss: 0.2265 (epoch: 5, step: 200) // Avg time/img: 0.0217 s\n",
            "loss: 0.2301 (epoch: 5, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.2301 (epoch: 5, step: 300) // Avg time/img: 0.0217 s\n",
            "loss: 0.2294 (epoch: 5, step: 350) // Avg time/img: 0.0217 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.2071 (epoch: 5, step: 0) // Avg time/img: 0.0122 s\n",
            "VAL loss: 0.3319 (epoch: 5, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m58.98\u001b[0m %\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.00038594475336178527\n",
            "loss: 0.231 (epoch: 6, step: 0) // Avg time/img: 0.0350 s\n",
            "loss: 0.207 (epoch: 6, step: 50) // Avg time/img: 0.0218 s\n",
            "loss: 0.2023 (epoch: 6, step: 100) // Avg time/img: 0.0218 s\n",
            "loss: 0.2059 (epoch: 6, step: 150) // Avg time/img: 0.0218 s\n",
            "loss: 0.2093 (epoch: 6, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.21 (epoch: 6, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.2106 (epoch: 6, step: 300) // Avg time/img: 0.0218 s\n",
            "loss: 0.2112 (epoch: 6, step: 350) // Avg time/img: 0.0218 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.2001 (epoch: 6, step: 0) // Avg time/img: 0.0154 s\n",
            "VAL loss: 0.3277 (epoch: 6, step: 50) // Avg time/img: 0.0111 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m62.38\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.00036270892346860996\n",
            "loss: 0.2709 (epoch: 7, step: 0) // Avg time/img: 0.0235 s\n",
            "loss: 0.1928 (epoch: 7, step: 50) // Avg time/img: 0.0221 s\n",
            "loss: 0.195 (epoch: 7, step: 100) // Avg time/img: 0.0220 s\n",
            "loss: 0.1995 (epoch: 7, step: 150) // Avg time/img: 0.0218 s\n",
            "loss: 0.2014 (epoch: 7, step: 200) // Avg time/img: 0.0216 s\n",
            "loss: 0.2016 (epoch: 7, step: 250) // Avg time/img: 0.0216 s\n",
            "loss: 0.2029 (epoch: 7, step: 300) // Avg time/img: 0.0216 s\n",
            "loss: 0.2077 (epoch: 7, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.4396 (epoch: 7, step: 0) // Avg time/img: 0.0123 s\n",
            "VAL loss: 0.6255 (epoch: 7, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m46.49\u001b[0m %\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0003393063796290625\n",
            "loss: 0.3168 (epoch: 8, step: 0) // Avg time/img: 0.0241 s\n",
            "loss: 0.2662 (epoch: 8, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.2408 (epoch: 8, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.2334 (epoch: 8, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.2316 (epoch: 8, step: 200) // Avg time/img: 0.0217 s\n",
            "loss: 0.2287 (epoch: 8, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.2243 (epoch: 8, step: 300) // Avg time/img: 0.0218 s\n",
            "loss: 0.2219 (epoch: 8, step: 350) // Avg time/img: 0.0218 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.2166 (epoch: 8, step: 0) // Avg time/img: 0.0121 s\n",
            "VAL loss: 0.3439 (epoch: 8, step: 50) // Avg time/img: 0.0100 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m59.01\u001b[0m %\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.00031572293374467766\n",
            "loss: 0.2279 (epoch: 9, step: 0) // Avg time/img: 0.0245 s\n",
            "loss: 0.1988 (epoch: 9, step: 50) // Avg time/img: 0.0217 s\n",
            "loss: 0.1943 (epoch: 9, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.1912 (epoch: 9, step: 150) // Avg time/img: 0.0215 s\n",
            "loss: 0.1904 (epoch: 9, step: 200) // Avg time/img: 0.0215 s\n",
            "loss: 0.1891 (epoch: 9, step: 250) // Avg time/img: 0.0215 s\n",
            "loss: 0.1901 (epoch: 9, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1911 (epoch: 9, step: 350) // Avg time/img: 0.0216 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.1993 (epoch: 9, step: 0) // Avg time/img: 0.0116 s\n",
            "VAL loss: 0.3157 (epoch: 9, step: 50) // Avg time/img: 0.0102 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m61.34\u001b[0m %\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.00029194189645999016\n",
            "loss: 0.1861 (epoch: 10, step: 0) // Avg time/img: 0.0238 s\n",
            "loss: 0.1885 (epoch: 10, step: 50) // Avg time/img: 0.0213 s\n",
            "loss: 0.1812 (epoch: 10, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.1822 (epoch: 10, step: 150) // Avg time/img: 0.0215 s\n",
            "loss: 0.1818 (epoch: 10, step: 200) // Avg time/img: 0.0215 s\n",
            "loss: 0.1801 (epoch: 10, step: 250) // Avg time/img: 0.0215 s\n",
            "loss: 0.1814 (epoch: 10, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.184 (epoch: 10, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.2046 (epoch: 10, step: 0) // Avg time/img: 0.0130 s\n",
            "VAL loss: 0.3344 (epoch: 10, step: 50) // Avg time/img: 0.0103 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m60.39\u001b[0m %\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0002679433656340733\n",
            "loss: 0.171 (epoch: 11, step: 0) // Avg time/img: 0.0291 s\n",
            "loss: 0.1775 (epoch: 11, step: 50) // Avg time/img: 0.0218 s\n",
            "loss: 0.1743 (epoch: 11, step: 100) // Avg time/img: 0.0219 s\n",
            "loss: 0.1741 (epoch: 11, step: 150) // Avg time/img: 0.0217 s\n",
            "loss: 0.1736 (epoch: 11, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.173 (epoch: 11, step: 250) // Avg time/img: 0.0218 s\n",
            "loss: 0.1732 (epoch: 11, step: 300) // Avg time/img: 0.0218 s\n",
            "loss: 0.1726 (epoch: 11, step: 350) // Avg time/img: 0.0218 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.1956 (epoch: 11, step: 0) // Avg time/img: 0.0110 s\n",
            "VAL loss: 0.3182 (epoch: 11, step: 50) // Avg time/img: 0.0111 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m64.43\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.00024370321958949772\n",
            "loss: 0.1322 (epoch: 12, step: 0) // Avg time/img: 0.0225 s\n",
            "loss: 0.1574 (epoch: 12, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.1596 (epoch: 12, step: 100) // Avg time/img: 0.0215 s\n",
            "loss: 0.1596 (epoch: 12, step: 150) // Avg time/img: 0.0214 s\n",
            "loss: 0.1629 (epoch: 12, step: 200) // Avg time/img: 0.0213 s\n",
            "loss: 0.1636 (epoch: 12, step: 250) // Avg time/img: 0.0214 s\n",
            "loss: 0.164 (epoch: 12, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1649 (epoch: 12, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.1947 (epoch: 12, step: 0) // Avg time/img: 0.0126 s\n",
            "VAL loss: 0.3587 (epoch: 12, step: 50) // Avg time/img: 0.0109 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m60.40\u001b[0m %\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.00021919164527704348\n",
            "loss: 0.1698 (epoch: 13, step: 0) // Avg time/img: 0.0235 s\n",
            "loss: 0.1647 (epoch: 13, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.1715 (epoch: 13, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.1785 (epoch: 13, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.1773 (epoch: 13, step: 200) // Avg time/img: 0.0217 s\n",
            "loss: 0.1741 (epoch: 13, step: 250) // Avg time/img: 0.0216 s\n",
            "loss: 0.1714 (epoch: 13, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1695 (epoch: 13, step: 350) // Avg time/img: 0.0216 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.1807 (epoch: 13, step: 0) // Avg time/img: 0.0129 s\n",
            "VAL loss: 0.2945 (epoch: 13, step: 50) // Avg time/img: 0.0105 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.61\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.00019437089939938174\n",
            "loss: 0.1638 (epoch: 14, step: 0) // Avg time/img: 0.0260 s\n",
            "loss: 0.1514 (epoch: 14, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.149 (epoch: 14, step: 100) // Avg time/img: 0.0215 s\n",
            "loss: 0.1485 (epoch: 14, step: 150) // Avg time/img: 0.0215 s\n",
            "loss: 0.1489 (epoch: 14, step: 200) // Avg time/img: 0.0215 s\n",
            "loss: 0.1489 (epoch: 14, step: 250) // Avg time/img: 0.0214 s\n",
            "loss: 0.1504 (epoch: 14, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1508 (epoch: 14, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.1627 (epoch: 14, step: 0) // Avg time/img: 0.0183 s\n",
            "VAL loss: 0.3109 (epoch: 14, step: 50) // Avg time/img: 0.0105 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.53\u001b[0m %\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.00016919173095082495\n",
            "loss: 0.1914 (epoch: 15, step: 0) // Avg time/img: 0.0264 s\n",
            "loss: 0.1475 (epoch: 15, step: 50) // Avg time/img: 0.0219 s\n",
            "loss: 0.1423 (epoch: 15, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.1433 (epoch: 15, step: 150) // Avg time/img: 0.0218 s\n",
            "loss: 0.1441 (epoch: 15, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.1444 (epoch: 15, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.1452 (epoch: 15, step: 300) // Avg time/img: 0.0217 s\n",
            "loss: 0.1456 (epoch: 15, step: 350) // Avg time/img: 0.0217 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.1667 (epoch: 15, step: 0) // Avg time/img: 0.0122 s\n",
            "VAL loss: 0.3102 (epoch: 15, step: 50) // Avg time/img: 0.0104 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.41\u001b[0m %\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.00014358729437462936\n",
            "loss: 0.1569 (epoch: 16, step: 0) // Avg time/img: 0.0243 s\n",
            "loss: 0.1363 (epoch: 16, step: 50) // Avg time/img: 0.0212 s\n",
            "loss: 0.1375 (epoch: 16, step: 100) // Avg time/img: 0.0214 s\n",
            "loss: 0.1396 (epoch: 16, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.1403 (epoch: 16, step: 200) // Avg time/img: 0.0215 s\n",
            "loss: 0.1406 (epoch: 16, step: 250) // Avg time/img: 0.0215 s\n",
            "loss: 0.1401 (epoch: 16, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1437 (epoch: 16, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.2002 (epoch: 16, step: 0) // Avg time/img: 0.0118 s\n",
            "VAL loss: 0.3426 (epoch: 16, step: 50) // Avg time/img: 0.0105 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m61.00\u001b[0m %\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.00011746189430880188\n",
            "loss: 0.1974 (epoch: 17, step: 0) // Avg time/img: 0.0246 s\n",
            "loss: 0.1521 (epoch: 17, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.151 (epoch: 17, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.1481 (epoch: 17, step: 150) // Avg time/img: 0.0215 s\n",
            "loss: 0.1463 (epoch: 17, step: 200) // Avg time/img: 0.0215 s\n",
            "loss: 0.144 (epoch: 17, step: 250) // Avg time/img: 0.0215 s\n",
            "loss: 0.1431 (epoch: 17, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.143 (epoch: 17, step: 350) // Avg time/img: 0.0214 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.1755 (epoch: 17, step: 0) // Avg time/img: 0.0125 s\n",
            "VAL loss: 0.3018 (epoch: 17, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.45\u001b[0m %\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683728e-05\n",
            "loss: 0.1274 (epoch: 18, step: 0) // Avg time/img: 0.0238 s\n",
            "loss: 0.1298 (epoch: 18, step: 50) // Avg time/img: 0.0214 s\n",
            "loss: 0.1287 (epoch: 18, step: 100) // Avg time/img: 0.0213 s\n",
            "loss: 0.1296 (epoch: 18, step: 150) // Avg time/img: 0.0213 s\n",
            "loss: 0.1305 (epoch: 18, step: 200) // Avg time/img: 0.0214 s\n",
            "loss: 0.1304 (epoch: 18, step: 250) // Avg time/img: 0.0214 s\n",
            "loss: 0.1302 (epoch: 18, step: 300) // Avg time/img: 0.0214 s\n",
            "loss: 0.1303 (epoch: 18, step: 350) // Avg time/img: 0.0214 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.1751 (epoch: 18, step: 0) // Avg time/img: 0.0128 s\n",
            "VAL loss: 0.2919 (epoch: 18, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.31\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-05\n",
            "loss: 0.1446 (epoch: 19, step: 0) // Avg time/img: 0.0251 s\n",
            "loss: 0.13 (epoch: 19, step: 50) // Avg time/img: 0.0218 s\n",
            "loss: 0.1274 (epoch: 19, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.1264 (epoch: 19, step: 150) // Avg time/img: 0.0217 s\n",
            "loss: 0.1262 (epoch: 19, step: 200) // Avg time/img: 0.0216 s\n",
            "loss: 0.1248 (epoch: 19, step: 250) // Avg time/img: 0.0216 s\n",
            "loss: 0.1249 (epoch: 19, step: 300) // Avg time/img: 0.0216 s\n",
            "loss: 0.1248 (epoch: 19, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.1687 (epoch: 19, step: 0) // Avg time/img: 0.0117 s\n",
            "VAL loss: 0.3039 (epoch: 19, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.56\u001b[0m %\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-05\n",
            "loss: 0.09816 (epoch: 20, step: 0) // Avg time/img: 0.0244 s\n",
            "loss: 0.1213 (epoch: 20, step: 50) // Avg time/img: 0.0216 s\n",
            "loss: 0.1217 (epoch: 20, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.1208 (epoch: 20, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.1203 (epoch: 20, step: 200) // Avg time/img: 0.0217 s\n",
            "loss: 0.1198 (epoch: 20, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.1197 (epoch: 20, step: 300) // Avg time/img: 0.0217 s\n",
            "loss: 0.1195 (epoch: 20, step: 350) // Avg time/img: 0.0217 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.173 (epoch: 20, step: 0) // Avg time/img: 0.0105 s\n",
            "VAL loss: 0.3112 (epoch: 20, step: 50) // Avg time/img: 0.0103 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.89\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "savedir = f'../save/BiSeNet_finetuned'\n",
        "if not os.path.exists(savedir):\n",
        "        os.makedirs(savedir)\n",
        "\n",
        "args = {\n",
        "    'height': 512,\n",
        "    'num_workers': 2,\n",
        "    'batch_size' : 8,\n",
        "    'cuda' : True,\n",
        "    'savedir' : savedir,\n",
        "    'resume': False,\n",
        "    'num_epochs': 30,\n",
        "    'steps_plot' : 50,\n",
        "    'visualize' : False,\n",
        "    'iouTrain': False,\n",
        "    'iouVal': True,\n",
        "    'steps_loss' : 50,\n",
        "    'epochs_save': 0,\n",
        "    'datadir': f'/content/Datasets/Cityscapes/',\n",
        "    'resume': True\n",
        "}\n",
        "\n",
        "\n",
        "bisenet_fine_tuned = train(args, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dypg0OHLW8G",
        "outputId": "22219454-485a-4c60-be3d-ad8d20e15d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Datasets/Cityscapes/leftImg8bit/train\n",
            "/content/Datasets/Cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:222: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n",
            "  warnings.warn(\"NLLLoss2d has been deprecated. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loaded checkpoint at epoch 21)\n",
            "----- TRAINING - EPOCH 21 -----\n",
            "LEARNING RATE:  0.00018602052900565077\n",
            "loss: 0.1141 (epoch: 21, step: 0) // Avg time/img: 0.0259 s\n",
            "loss: 0.1211 (epoch: 21, step: 50) // Avg time/img: 0.0216 s\n",
            "loss: 0.1265 (epoch: 21, step: 100) // Avg time/img: 0.0214 s\n",
            "loss: 0.1293 (epoch: 21, step: 150) // Avg time/img: 0.0214 s\n",
            "loss: 0.1405 (epoch: 21, step: 200) // Avg time/img: 0.0214 s\n",
            "loss: 0.1524 (epoch: 21, step: 250) // Avg time/img: 0.0214 s\n",
            "loss: 0.168 (epoch: 21, step: 300) // Avg time/img: 0.0214 s\n",
            "loss: 0.1726 (epoch: 21, step: 350) // Avg time/img: 0.0214 s\n",
            "----- VALIDATING - EPOCH 21 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-1e75ccddfce5>:277: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "<ipython-input-95-1e75ccddfce5>:278: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL loss: 0.2243 (epoch: 21, step: 0) // Avg time/img: 0.0136 s\n",
            "VAL loss: 0.34 (epoch: 21, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m58.93\u001b[0m %\n",
            "----- TRAINING - EPOCH 22 -----\n",
            "LEARNING RATE:  0.00016919173095082495\n",
            "loss: 0.2138 (epoch: 22, step: 0) // Avg time/img: 0.0250 s\n",
            "loss: 0.1795 (epoch: 22, step: 50) // Avg time/img: 0.0218 s\n",
            "loss: 0.1745 (epoch: 22, step: 100) // Avg time/img: 0.0215 s\n",
            "loss: 0.1719 (epoch: 22, step: 150) // Avg time/img: 0.0214 s\n",
            "loss: 0.1698 (epoch: 22, step: 200) // Avg time/img: 0.0214 s\n",
            "loss: 0.1663 (epoch: 22, step: 250) // Avg time/img: 0.0213 s\n",
            "loss: 0.1649 (epoch: 22, step: 300) // Avg time/img: 0.0213 s\n",
            "loss: 0.1637 (epoch: 22, step: 350) // Avg time/img: 0.0213 s\n",
            "----- VALIDATING - EPOCH 22 -----\n",
            "VAL loss: 0.1946 (epoch: 22, step: 0) // Avg time/img: 0.0083 s\n",
            "VAL loss: 0.3069 (epoch: 22, step: 50) // Avg time/img: 0.0103 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m64.72\u001b[0m %\n",
            "----- TRAINING - EPOCH 23 -----\n",
            "LEARNING RATE:  0.00015217449296258857\n",
            "loss: 0.1428 (epoch: 23, step: 0) // Avg time/img: 0.0255 s\n",
            "loss: 0.1487 (epoch: 23, step: 50) // Avg time/img: 0.0215 s\n",
            "loss: 0.1451 (epoch: 23, step: 100) // Avg time/img: 0.0218 s\n",
            "loss: 0.143 (epoch: 23, step: 150) // Avg time/img: 0.0218 s\n",
            "loss: 0.1428 (epoch: 23, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.1417 (epoch: 23, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.1405 (epoch: 23, step: 300) // Avg time/img: 0.0216 s\n",
            "loss: 0.1401 (epoch: 23, step: 350) // Avg time/img: 0.0216 s\n",
            "----- VALIDATING - EPOCH 23 -----\n",
            "VAL loss: 0.1667 (epoch: 23, step: 0) // Avg time/img: 0.0125 s\n",
            "VAL loss: 0.3116 (epoch: 23, step: 50) // Avg time/img: 0.0105 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.50\u001b[0m %\n",
            "----- TRAINING - EPOCH 24 -----\n",
            "LEARNING RATE:  0.00013494261163740181\n",
            "loss: 0.1172 (epoch: 24, step: 0) // Avg time/img: 0.0219 s\n",
            "loss: 0.1309 (epoch: 24, step: 50) // Avg time/img: 0.0216 s\n",
            "loss: 0.1308 (epoch: 24, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.1312 (epoch: 24, step: 150) // Avg time/img: 0.0218 s\n",
            "loss: 0.1304 (epoch: 24, step: 200) // Avg time/img: 0.0217 s\n",
            "loss: 0.13 (epoch: 24, step: 250) // Avg time/img: 0.0216 s\n",
            "loss: 0.1294 (epoch: 24, step: 300) // Avg time/img: 0.0216 s\n",
            "loss: 0.1293 (epoch: 24, step: 350) // Avg time/img: 0.0216 s\n",
            "----- VALIDATING - EPOCH 24 -----\n",
            "VAL loss: 0.1866 (epoch: 24, step: 0) // Avg time/img: 0.0144 s\n",
            "VAL loss: 0.3068 (epoch: 24, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.72\u001b[0m %\n",
            "----- TRAINING - EPOCH 25 -----\n",
            "LEARNING RATE:  0.00011746189430880188\n",
            "loss: 0.1358 (epoch: 25, step: 0) // Avg time/img: 0.0262 s\n",
            "loss: 0.1276 (epoch: 25, step: 50) // Avg time/img: 0.0219 s\n",
            "loss: 0.1265 (epoch: 25, step: 100) // Avg time/img: 0.0219 s\n",
            "loss: 0.1259 (epoch: 25, step: 150) // Avg time/img: 0.0219 s\n",
            "loss: 0.1253 (epoch: 25, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.1257 (epoch: 25, step: 250) // Avg time/img: 0.0218 s\n",
            "loss: 0.1251 (epoch: 25, step: 300) // Avg time/img: 0.0217 s\n",
            "loss: 0.1255 (epoch: 25, step: 350) // Avg time/img: 0.0217 s\n",
            "----- VALIDATING - EPOCH 25 -----\n",
            "VAL loss: 0.1835 (epoch: 25, step: 0) // Avg time/img: 0.0126 s\n",
            "VAL loss: 0.3004 (epoch: 25, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m65.28\u001b[0m %\n",
            "----- TRAINING - EPOCH 26 -----\n",
            "LEARNING RATE:  9.96859332376096e-05\n",
            "loss: 0.129 (epoch: 26, step: 0) // Avg time/img: 0.0313 s\n",
            "loss: 0.1212 (epoch: 26, step: 50) // Avg time/img: 0.0223 s\n",
            "loss: 0.1209 (epoch: 26, step: 100) // Avg time/img: 0.0222 s\n",
            "loss: 0.1214 (epoch: 26, step: 150) // Avg time/img: 0.0219 s\n",
            "loss: 0.1217 (epoch: 26, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.1214 (epoch: 26, step: 250) // Avg time/img: 0.0218 s\n",
            "loss: 0.1216 (epoch: 26, step: 300) // Avg time/img: 0.0217 s\n",
            "loss: 0.1206 (epoch: 26, step: 350) // Avg time/img: 0.0217 s\n",
            "----- VALIDATING - EPOCH 26 -----\n",
            "VAL loss: 0.176 (epoch: 26, step: 0) // Avg time/img: 0.0124 s\n",
            "VAL loss: 0.3278 (epoch: 26, step: 50) // Avg time/img: 0.0105 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.03\u001b[0m %\n",
            "----- TRAINING - EPOCH 27 -----\n",
            "LEARNING RATE:  8.154829161610912e-05\n",
            "loss: 0.1385 (epoch: 27, step: 0) // Avg time/img: 0.0266 s\n",
            "loss: 0.1165 (epoch: 27, step: 50) // Avg time/img: 0.0218 s\n",
            "loss: 0.1164 (epoch: 27, step: 100) // Avg time/img: 0.0218 s\n",
            "loss: 0.1179 (epoch: 27, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.1187 (epoch: 27, step: 200) // Avg time/img: 0.0216 s\n",
            "loss: 0.1183 (epoch: 27, step: 250) // Avg time/img: 0.0215 s\n",
            "loss: 0.1177 (epoch: 27, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1174 (epoch: 27, step: 350) // Avg time/img: 0.0216 s\n",
            "----- VALIDATING - EPOCH 27 -----\n",
            "VAL loss: 0.1797 (epoch: 27, step: 0) // Avg time/img: 0.0112 s\n",
            "VAL loss: 0.3187 (epoch: 27, step: 50) // Avg time/img: 0.0107 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.42\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 27)\n",
            "----- TRAINING - EPOCH 28 -----\n",
            "LEARNING RATE:  6.294627058970836e-05\n",
            "loss: 0.09101 (epoch: 28, step: 0) // Avg time/img: 0.0250 s\n",
            "loss: 0.1165 (epoch: 28, step: 50) // Avg time/img: 0.0216 s\n",
            "loss: 0.1157 (epoch: 28, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.1145 (epoch: 28, step: 150) // Avg time/img: 0.0218 s\n",
            "loss: 0.1147 (epoch: 28, step: 200) // Avg time/img: 0.0218 s\n",
            "loss: 0.1139 (epoch: 28, step: 250) // Avg time/img: 0.0217 s\n",
            "loss: 0.114 (epoch: 28, step: 300) // Avg time/img: 0.0217 s\n",
            "loss: 0.1135 (epoch: 28, step: 350) // Avg time/img: 0.0217 s\n",
            "----- VALIDATING - EPOCH 28 -----\n",
            "VAL loss: 0.1711 (epoch: 28, step: 0) // Avg time/img: 0.0140 s\n",
            "VAL loss: 0.324 (epoch: 28, step: 50) // Avg time/img: 0.0109 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.71\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 28)\n",
            "----- TRAINING - EPOCH 29 -----\n",
            "LEARNING RATE:  4.370064743465832e-05\n",
            "loss: 0.1002 (epoch: 29, step: 0) // Avg time/img: 0.0271 s\n",
            "loss: 0.1092 (epoch: 29, step: 50) // Avg time/img: 0.0216 s\n",
            "loss: 0.1107 (epoch: 29, step: 100) // Avg time/img: 0.0217 s\n",
            "loss: 0.1079 (epoch: 29, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.1073 (epoch: 29, step: 200) // Avg time/img: 0.0216 s\n",
            "loss: 0.1083 (epoch: 29, step: 250) // Avg time/img: 0.0215 s\n",
            "loss: 0.109 (epoch: 29, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1094 (epoch: 29, step: 350) // Avg time/img: 0.0215 s\n",
            "----- VALIDATING - EPOCH 29 -----\n",
            "VAL loss: 0.1897 (epoch: 29, step: 0) // Avg time/img: 0.0123 s\n",
            "VAL loss: 0.3218 (epoch: 29, step: 50) // Avg time/img: 0.0103 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 29)\n",
            "----- TRAINING - EPOCH 30 -----\n",
            "LEARNING RATE:  2.3418597108060762e-05\n",
            "loss: 0.1194 (epoch: 30, step: 0) // Avg time/img: 0.0271 s\n",
            "loss: 0.1083 (epoch: 30, step: 50) // Avg time/img: 0.0213 s\n",
            "loss: 0.1068 (epoch: 30, step: 100) // Avg time/img: 0.0216 s\n",
            "loss: 0.1073 (epoch: 30, step: 150) // Avg time/img: 0.0216 s\n",
            "loss: 0.1076 (epoch: 30, step: 200) // Avg time/img: 0.0215 s\n",
            "loss: 0.1065 (epoch: 30, step: 250) // Avg time/img: 0.0216 s\n",
            "loss: 0.1069 (epoch: 30, step: 300) // Avg time/img: 0.0215 s\n",
            "loss: 0.1063 (epoch: 30, step: 350) // Avg time/img: 0.0216 s\n",
            "----- VALIDATING - EPOCH 30 -----\n",
            "VAL loss: 0.185 (epoch: 30, step: 0) // Avg time/img: 0.0134 s\n",
            "VAL loss: 0.3222 (epoch: 30, step: 50) // Avg time/img: 0.0104 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.30\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  splits = name.split('.')\n",
        "  if str(splits[0]+'.'+splits[1]) in keys:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "qglMatBga0hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savedir = f'../save/BiSeNet_finetuned'\n",
        "if not os.path.exists(savedir):\n",
        "        os.makedirs(savedir)\n",
        "\n",
        "args = {\n",
        "    'height': 512,\n",
        "    'num_workers': 2,\n",
        "    'batch_size' : 8,\n",
        "    'cuda' : True,\n",
        "    'savedir' : savedir,\n",
        "    'resume': False,\n",
        "    'num_epochs': 35,\n",
        "    'steps_plot' : 50,\n",
        "    'visualize' : False,\n",
        "    'iouTrain': False,\n",
        "    'iouVal': True,\n",
        "    'steps_loss' : 50,\n",
        "    'epochs_save': 0,\n",
        "    'datadir': f'/content/Datasets/Cityscapes/',\n",
        "    'resume': True\n",
        "}\n",
        "\n",
        "\n",
        "bisenet_fine_tuned = train(args, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFTaumu6a8C3",
        "outputId": "c0497caf-0fac-407e-d823-438df45dfac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Datasets/Cityscapes/leftImg8bit/train\n",
            "/content/Datasets/Cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "=> Loaded checkpoint at epoch 31)\n",
            "----- TRAINING - EPOCH 31 -----\n",
            "LEARNING RATE:  8.677243171707623e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:222: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n",
            "  warnings.warn(\"NLLLoss2d has been deprecated. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.1205 (epoch: 31, step: 0) // Avg time/img: 0.0112 s\n",
            "loss: 0.1057 (epoch: 31, step: 50) // Avg time/img: 0.0125 s\n",
            "loss: 0.1058 (epoch: 31, step: 100) // Avg time/img: 0.0122 s\n",
            "loss: 0.1053 (epoch: 31, step: 150) // Avg time/img: 0.0121 s\n",
            "loss: 0.1048 (epoch: 31, step: 200) // Avg time/img: 0.0123 s\n",
            "loss: 0.1039 (epoch: 31, step: 250) // Avg time/img: 0.0123 s\n",
            "loss: 0.1041 (epoch: 31, step: 300) // Avg time/img: 0.0123 s\n",
            "loss: 0.1043 (epoch: 31, step: 350) // Avg time/img: 0.0123 s\n",
            "----- VALIDATING - EPOCH 31 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-1e75ccddfce5>:277: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "<ipython-input-95-1e75ccddfce5>:278: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL loss: 0.1857 (epoch: 31, step: 0) // Avg time/img: 0.0114 s\n",
            "VAL loss: 0.3214 (epoch: 31, step: 50) // Avg time/img: 0.0113 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.54\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 31)\n",
            "----- TRAINING - EPOCH 32 -----\n",
            "LEARNING RATE:  7.098437398420578e-05\n",
            "loss: 0.08686 (epoch: 32, step: 0) // Avg time/img: 0.0144 s\n",
            "loss: 0.1032 (epoch: 32, step: 50) // Avg time/img: 0.0125 s\n",
            "loss: 0.1031 (epoch: 32, step: 100) // Avg time/img: 0.0124 s\n",
            "loss: 0.1039 (epoch: 32, step: 150) // Avg time/img: 0.0123 s\n",
            "loss: 0.1045 (epoch: 32, step: 200) // Avg time/img: 0.0120 s\n",
            "loss: 0.1047 (epoch: 32, step: 250) // Avg time/img: 0.0120 s\n",
            "loss: 0.1047 (epoch: 32, step: 300) // Avg time/img: 0.0119 s\n",
            "loss: 0.1046 (epoch: 32, step: 350) // Avg time/img: 0.0118 s\n",
            "----- VALIDATING - EPOCH 32 -----\n",
            "VAL loss: 0.1853 (epoch: 32, step: 0) // Avg time/img: 0.0115 s\n",
            "VAL loss: 0.3245 (epoch: 32, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.56\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 32)\n",
            "----- TRAINING - EPOCH 33 -----\n",
            "LEARNING RATE:  5.479209341975005e-05\n",
            "loss: 0.1074 (epoch: 33, step: 0) // Avg time/img: 0.0130 s\n",
            "loss: 0.1034 (epoch: 33, step: 50) // Avg time/img: 0.0123 s\n",
            "loss: 0.1035 (epoch: 33, step: 100) // Avg time/img: 0.0123 s\n",
            "loss: 0.1037 (epoch: 33, step: 150) // Avg time/img: 0.0121 s\n",
            "loss: 0.1037 (epoch: 33, step: 200) // Avg time/img: 0.0121 s\n",
            "loss: 0.104 (epoch: 33, step: 250) // Avg time/img: 0.0120 s\n",
            "loss: 0.1044 (epoch: 33, step: 300) // Avg time/img: 0.0119 s\n",
            "loss: 0.1042 (epoch: 33, step: 350) // Avg time/img: 0.0119 s\n",
            "----- VALIDATING - EPOCH 33 -----\n",
            "VAL loss: 0.186 (epoch: 33, step: 0) // Avg time/img: 0.0128 s\n",
            "VAL loss: 0.3253 (epoch: 33, step: 50) // Avg time/img: 0.0113 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.44\u001b[0m %\n",
            "----- TRAINING - EPOCH 34 -----\n",
            "LEARNING RATE:  3.803958414551169e-05\n",
            "loss: 0.1048 (epoch: 34, step: 0) // Avg time/img: 0.0152 s\n",
            "loss: 0.1042 (epoch: 34, step: 50) // Avg time/img: 0.0126 s\n",
            "loss: 0.1028 (epoch: 34, step: 100) // Avg time/img: 0.0124 s\n",
            "loss: 0.1037 (epoch: 34, step: 150) // Avg time/img: 0.0123 s\n",
            "loss: 0.1042 (epoch: 34, step: 200) // Avg time/img: 0.0121 s\n",
            "loss: 0.1046 (epoch: 34, step: 250) // Avg time/img: 0.0121 s\n",
            "loss: 0.1048 (epoch: 34, step: 300) // Avg time/img: 0.0121 s\n",
            "loss: 0.1044 (epoch: 34, step: 350) // Avg time/img: 0.0121 s\n",
            "----- VALIDATING - EPOCH 34 -----\n",
            "VAL loss: 0.1846 (epoch: 34, step: 0) // Avg time/img: 0.0139 s\n",
            "VAL loss: 0.3281 (epoch: 34, step: 50) // Avg time/img: 0.0108 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.49\u001b[0m %\n",
            "----- TRAINING - EPOCH 35 -----\n",
            "LEARNING RATE:  2.0384908406537878e-05\n",
            "loss: 0.1142 (epoch: 35, step: 0) // Avg time/img: 0.0137 s\n",
            "loss: 0.1047 (epoch: 35, step: 50) // Avg time/img: 0.0123 s\n",
            "loss: 0.1046 (epoch: 35, step: 100) // Avg time/img: 0.0126 s\n",
            "loss: 0.1046 (epoch: 35, step: 150) // Avg time/img: 0.0125 s\n",
            "loss: 0.1049 (epoch: 35, step: 200) // Avg time/img: 0.0126 s\n",
            "loss: 0.1052 (epoch: 35, step: 250) // Avg time/img: 0.0126 s\n",
            "loss: 0.1046 (epoch: 35, step: 300) // Avg time/img: 0.0125 s\n",
            "loss: 0.1043 (epoch: 35, step: 350) // Avg time/img: 0.0124 s\n",
            "----- VALIDATING - EPOCH 35 -----\n",
            "VAL loss: 0.1858 (epoch: 35, step: 0) // Avg time/img: 0.0111 s\n",
            "VAL loss: 0.324 (epoch: 35, step: 50) // Avg time/img: 0.0103 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m67.66\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/BiSeNet_finetuned/model_best.pth (epoch: 35)\n"
          ]
        }
      ]
    }
  ]
}