{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AML\n",
        "!git clone https://github.com/m-depascale/AnomalySegmentation_AML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNG7S2eGPj1i",
        "outputId": "e84c9502-9712-446f-ad07-b4b0d9b7d844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/AML'\n",
            "/content\n",
            "Cloning into 'AnomalySegmentation_AML'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 336 (delta 8), reused 13 (delta 8), pack-reused 320\u001b[K\n",
            "Receiving objects: 100% (336/336), 185.59 MiB | 29.52 MiB/s, done.\n",
            "Resolving deltas: 100% (172/172), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ood-metrics\n",
        "!pip install visdom\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install Pillow\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTVlXBPlPpPW",
        "outputId": "2238b44b-6e0a-49f6-ac9a-ed4a860f4f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ood-metrics\n",
            "  Downloading ood_metrics-1.1.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from ood-metrics) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.22 in /usr/local/lib/python3.10/dist-packages (from ood-metrics) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from ood-metrics) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood-metrics) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood-metrics) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.0->ood-metrics) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>=3.0->ood-metrics) (1.16.0)\n",
            "Installing collected packages: ood-metrics\n",
            "Successfully installed ood-metrics-1.1.2\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.16.0)\n",
            "Collecting jsonpatch (from visdom)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.7.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.2.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (9.4.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2024.2.2)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408194 sha256=adba54b72120b0db9d27c87a5a98325d2ad19c94dcbfc586ae5ba7d534fdaa7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built visdom\n",
            "Installing collected packages: jsonpointer, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 visdom-0.2.4\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pretrained erfnet\n"
      ],
      "metadata": {
        "id": "LQasBnQTM14d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AnomalySegmentation_AML/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1m1-pgqP5l1",
        "outputId": "45a4f276-a664-4de9-f575-fbc8ea0c204b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AnomalySegmentation_AML/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7_rp2VSMow8",
        "outputId": "970dedeb-04b6-466c-c6ef-2970233d27d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ERFNet(\n",
              "  (encoder): Encoder(\n",
              "    (initial_block): DownsamplerBlock(\n",
              "      (conv): Conv2d(3, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): DownsamplerBlock(\n",
              "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1-5): 5 x non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): DownsamplerBlock(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (7): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(2, 0), dilation=(2, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2), dilation=(1, 2))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(4, 0), dilation=(4, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 4), dilation=(1, 4))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(8, 0), dilation=(8, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 8), dilation=(1, 8))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(16, 0), dilation=(16, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 16), dilation=(1, 16))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(2, 0), dilation=(2, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2), dilation=(1, 2))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (12): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(4, 0), dilation=(4, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 4), dilation=(1, 4))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (13): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(8, 0), dilation=(8, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 8), dilation=(1, 8))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "      (14): non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(16, 0), dilation=(16, 1))\n",
              "        (conv1x3_2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 16), dilation=(1, 16))\n",
              "        (bn2): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): UpsamplerBlock(\n",
              "        (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1-2): 2 x non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0, inplace=False)\n",
              "      )\n",
              "      (3): UpsamplerBlock(\n",
              "        (conv): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4-5): 2 x non_bottleneck_1d(\n",
              "        (conv3x1_1): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_1): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3x1_2): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (conv1x3_2): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (dropout): Dropout2d(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (output_conv): ConvTranspose2d(16, 20, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from erfnet import ERFNet\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "# Create an instance of BiSeNet model\n",
        "\n",
        "def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                if name.startswith(\"module.\"):\n",
        "                    own_state[name.split(\"module.\")[-1]].copy_(param)\n",
        "                else:\n",
        "                    print(name, \" not loaded\")\n",
        "                    continue\n",
        "            else:\n",
        "                own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "erf = ERFNet(20)\n",
        "model = load_my_state_dict(erf, torch.load('/content/AnomalySegmentation_AML/trained_models/erfnet_pretrained.pth')) #, map_location=lambda storage, loc: storage))\n",
        "\n",
        "# Use GPU\n",
        "device = torch.device(\"cuda\") # \"cpu\" or \"cuda\"\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSRYTWqcQoNa",
        "outputId": "0793a67c-2662-4919-aed0-c9da02c754d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "encoder\n",
            "encoder.initial_block\n",
            "encoder.initial_block.conv\n",
            "encoder.initial_block.pool\n",
            "encoder.initial_block.bn\n",
            "encoder.layers\n",
            "encoder.layers.0\n",
            "encoder.layers.0.conv\n",
            "encoder.layers.0.pool\n",
            "encoder.layers.0.bn\n",
            "encoder.layers.1\n",
            "encoder.layers.1.conv3x1_1\n",
            "encoder.layers.1.conv1x3_1\n",
            "encoder.layers.1.bn1\n",
            "encoder.layers.1.conv3x1_2\n",
            "encoder.layers.1.conv1x3_2\n",
            "encoder.layers.1.bn2\n",
            "encoder.layers.1.dropout\n",
            "encoder.layers.2\n",
            "encoder.layers.2.conv3x1_1\n",
            "encoder.layers.2.conv1x3_1\n",
            "encoder.layers.2.bn1\n",
            "encoder.layers.2.conv3x1_2\n",
            "encoder.layers.2.conv1x3_2\n",
            "encoder.layers.2.bn2\n",
            "encoder.layers.2.dropout\n",
            "encoder.layers.3\n",
            "encoder.layers.3.conv3x1_1\n",
            "encoder.layers.3.conv1x3_1\n",
            "encoder.layers.3.bn1\n",
            "encoder.layers.3.conv3x1_2\n",
            "encoder.layers.3.conv1x3_2\n",
            "encoder.layers.3.bn2\n",
            "encoder.layers.3.dropout\n",
            "encoder.layers.4\n",
            "encoder.layers.4.conv3x1_1\n",
            "encoder.layers.4.conv1x3_1\n",
            "encoder.layers.4.bn1\n",
            "encoder.layers.4.conv3x1_2\n",
            "encoder.layers.4.conv1x3_2\n",
            "encoder.layers.4.bn2\n",
            "encoder.layers.4.dropout\n",
            "encoder.layers.5\n",
            "encoder.layers.5.conv3x1_1\n",
            "encoder.layers.5.conv1x3_1\n",
            "encoder.layers.5.bn1\n",
            "encoder.layers.5.conv3x1_2\n",
            "encoder.layers.5.conv1x3_2\n",
            "encoder.layers.5.bn2\n",
            "encoder.layers.5.dropout\n",
            "encoder.layers.6\n",
            "encoder.layers.6.conv\n",
            "encoder.layers.6.pool\n",
            "encoder.layers.6.bn\n",
            "encoder.layers.7\n",
            "encoder.layers.7.conv3x1_1\n",
            "encoder.layers.7.conv1x3_1\n",
            "encoder.layers.7.bn1\n",
            "encoder.layers.7.conv3x1_2\n",
            "encoder.layers.7.conv1x3_2\n",
            "encoder.layers.7.bn2\n",
            "encoder.layers.7.dropout\n",
            "encoder.layers.8\n",
            "encoder.layers.8.conv3x1_1\n",
            "encoder.layers.8.conv1x3_1\n",
            "encoder.layers.8.bn1\n",
            "encoder.layers.8.conv3x1_2\n",
            "encoder.layers.8.conv1x3_2\n",
            "encoder.layers.8.bn2\n",
            "encoder.layers.8.dropout\n",
            "encoder.layers.9\n",
            "encoder.layers.9.conv3x1_1\n",
            "encoder.layers.9.conv1x3_1\n",
            "encoder.layers.9.bn1\n",
            "encoder.layers.9.conv3x1_2\n",
            "encoder.layers.9.conv1x3_2\n",
            "encoder.layers.9.bn2\n",
            "encoder.layers.9.dropout\n",
            "encoder.layers.10\n",
            "encoder.layers.10.conv3x1_1\n",
            "encoder.layers.10.conv1x3_1\n",
            "encoder.layers.10.bn1\n",
            "encoder.layers.10.conv3x1_2\n",
            "encoder.layers.10.conv1x3_2\n",
            "encoder.layers.10.bn2\n",
            "encoder.layers.10.dropout\n",
            "encoder.layers.11\n",
            "encoder.layers.11.conv3x1_1\n",
            "encoder.layers.11.conv1x3_1\n",
            "encoder.layers.11.bn1\n",
            "encoder.layers.11.conv3x1_2\n",
            "encoder.layers.11.conv1x3_2\n",
            "encoder.layers.11.bn2\n",
            "encoder.layers.11.dropout\n",
            "encoder.layers.12\n",
            "encoder.layers.12.conv3x1_1\n",
            "encoder.layers.12.conv1x3_1\n",
            "encoder.layers.12.bn1\n",
            "encoder.layers.12.conv3x1_2\n",
            "encoder.layers.12.conv1x3_2\n",
            "encoder.layers.12.bn2\n",
            "encoder.layers.12.dropout\n",
            "encoder.layers.13\n",
            "encoder.layers.13.conv3x1_1\n",
            "encoder.layers.13.conv1x3_1\n",
            "encoder.layers.13.bn1\n",
            "encoder.layers.13.conv3x1_2\n",
            "encoder.layers.13.conv1x3_2\n",
            "encoder.layers.13.bn2\n",
            "encoder.layers.13.dropout\n",
            "encoder.layers.14\n",
            "encoder.layers.14.conv3x1_1\n",
            "encoder.layers.14.conv1x3_1\n",
            "encoder.layers.14.bn1\n",
            "encoder.layers.14.conv3x1_2\n",
            "encoder.layers.14.conv1x3_2\n",
            "encoder.layers.14.bn2\n",
            "encoder.layers.14.dropout\n",
            "encoder.output_conv\n",
            "decoder\n",
            "decoder.layers\n",
            "decoder.layers.0\n",
            "decoder.layers.0.conv\n",
            "decoder.layers.0.bn\n",
            "decoder.layers.1\n",
            "decoder.layers.1.conv3x1_1\n",
            "decoder.layers.1.conv1x3_1\n",
            "decoder.layers.1.bn1\n",
            "decoder.layers.1.conv3x1_2\n",
            "decoder.layers.1.conv1x3_2\n",
            "decoder.layers.1.bn2\n",
            "decoder.layers.1.dropout\n",
            "decoder.layers.2\n",
            "decoder.layers.2.conv3x1_1\n",
            "decoder.layers.2.conv1x3_1\n",
            "decoder.layers.2.bn1\n",
            "decoder.layers.2.conv3x1_2\n",
            "decoder.layers.2.conv1x3_2\n",
            "decoder.layers.2.bn2\n",
            "decoder.layers.2.dropout\n",
            "decoder.layers.3\n",
            "decoder.layers.3.conv\n",
            "decoder.layers.3.bn\n",
            "decoder.layers.4\n",
            "decoder.layers.4.conv3x1_1\n",
            "decoder.layers.4.conv1x3_1\n",
            "decoder.layers.4.bn1\n",
            "decoder.layers.4.conv3x1_2\n",
            "decoder.layers.4.conv1x3_2\n",
            "decoder.layers.4.bn2\n",
            "decoder.layers.4.dropout\n",
            "decoder.layers.5\n",
            "decoder.layers.5.conv3x1_1\n",
            "decoder.layers.5.conv1x3_1\n",
            "decoder.layers.5.bn1\n",
            "decoder.layers.5.conv3x1_2\n",
            "decoder.layers.5.conv1x3_2\n",
            "decoder.layers.5.bn2\n",
            "decoder.layers.5.dropout\n",
            "decoder.output_conv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "uw3ejZEtNPUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AnomalySegmentation_AML/train\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.optim import SGD, Adam, lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize, Pad\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import VOC12,cityscapes\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "from visualize import Dashboard\n",
        "\n",
        "import importlib\n",
        "from iouEval import iouEval, getColorEntry\n",
        "\n",
        "from shutil import copyfile\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20 #pascal=22, cityscapes=20\n",
        "\n",
        "color_transform = Colorize(NUM_CLASSES)\n",
        "image_transform = ToPILImage()\n",
        "\n",
        "#Augmentations - different function implemented to perform random augments on both image and target\n",
        "class MyCoTransform(object):\n",
        "    def __init__(self, enc, augment=True, height=512):\n",
        "        self.enc=enc\n",
        "        self.augment = augment\n",
        "        self.height = height\n",
        "        pass\n",
        "    def __call__(self, input, target):\n",
        "        # do something to both images\n",
        "        input =  Resize(self.height, Image.BILINEAR)(input)\n",
        "        target = Resize(self.height, Image.NEAREST)(target)\n",
        "\n",
        "        if(self.augment):\n",
        "            # Random hflip\n",
        "            hflip = random.random()\n",
        "            if (hflip < 0.5):\n",
        "                input = input.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                target = target.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            #Random translation 0-2 pixels (fill rest with padding\n",
        "            transX = random.randint(-2, 2)\n",
        "            transY = random.randint(-2, 2)\n",
        "\n",
        "            input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)\n",
        "            target = ImageOps.expand(target, border=(transX,transY,0,0), fill=255) #pad label filling with 255\n",
        "            input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))\n",
        "            target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))\n",
        "\n",
        "        input = ToTensor()(input)\n",
        "        if (self.enc):\n",
        "            target = Resize(int(self.height/8), Image.NEAREST)(target)\n",
        "        target = ToLabel()(target)\n",
        "        target = Relabel(255, 19)(target)\n",
        "\n",
        "        return input, target\n",
        "\n",
        "\n",
        "class CrossEntropyLoss2d(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss = torch.nn.NLLLoss2d(weight)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        return self.loss(torch.nn.functional.log_softmax(outputs, dim=1), targets)\n",
        "\n",
        "\n",
        "def train(args, model, enc=False):\n",
        "    best_acc = 0\n",
        "\n",
        "    #TODO: calculate weights by processing dataset histogram (now its being set by hand from the torch values)\n",
        "    #create a loder to run all images and calculate histogram of labels, then create weight array using class balancing\n",
        "\n",
        "    weight = torch.ones(NUM_CLASSES)\n",
        "    if (enc):\n",
        "        weight[0] = 2.3653597831726\n",
        "        weight[1] = 4.4237880706787\n",
        "        weight[2] = 2.9691488742828\n",
        "        weight[3] = 5.3442072868347\n",
        "        weight[4] = 5.2983593940735\n",
        "        weight[5] = 5.2275490760803\n",
        "        weight[6] = 5.4394111633301\n",
        "        weight[7] = 5.3659925460815\n",
        "        weight[8] = 3.4170460700989\n",
        "        weight[9] = 5.2414722442627\n",
        "        weight[10] = 4.7376127243042\n",
        "        weight[11] = 5.2286224365234\n",
        "        weight[12] = 5.455126285553\n",
        "        weight[13] = 4.3019247055054\n",
        "        weight[14] = 5.4264230728149\n",
        "        weight[15] = 5.4331531524658\n",
        "        weight[16] = 5.433765411377\n",
        "        weight[17] = 5.4631009101868\n",
        "        weight[18] = 5.3947434425354\n",
        "    else:\n",
        "        weight[0] = 2.8149201869965\n",
        "        weight[1] = 6.9850029945374\n",
        "        weight[2] = 3.7890393733978\n",
        "        weight[3] = 9.9428062438965\n",
        "        weight[4] = 9.7702074050903\n",
        "        weight[5] = 9.5110931396484\n",
        "        weight[6] = 10.311357498169\n",
        "        weight[7] = 10.026463508606\n",
        "        weight[8] = 4.6323022842407\n",
        "        weight[9] = 9.5608062744141\n",
        "        weight[10] = 7.8698215484619\n",
        "        weight[11] = 9.5168733596802\n",
        "        weight[12] = 10.373730659485\n",
        "        weight[13] = 6.6616044044495\n",
        "        weight[14] = 10.260489463806\n",
        "        weight[15] = 10.287888526917\n",
        "        weight[16] = 10.289801597595\n",
        "        weight[17] = 10.405355453491\n",
        "        weight[18] = 10.138095855713\n",
        "\n",
        "    weight[19] = 1.0\n",
        "\n",
        "    assert os.path.exists(args['datadir']), \"Error: datadir (dataset directory) could not be loaded\"\n",
        "\n",
        "    co_transform = MyCoTransform(enc, augment=True, height=args['height'])#1024)\n",
        "    co_transform_val = MyCoTransform(enc, augment=False, height=args['height'])#1024)\n",
        "    dataset_train = cityscapes(args['datadir'], co_transform, 'train')\n",
        "    dataset_val = cityscapes(args['datadir'], co_transform_val, 'val')\n",
        "\n",
        "    loader = DataLoader(dataset_train, num_workers=args['num_workers'], batch_size=args['batch_size'], shuffle=True)\n",
        "    loader_val = DataLoader(dataset_val, num_workers=args['num_workers'], batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "    if args['cuda']:\n",
        "        weight = weight.cuda()\n",
        "    criterion = CrossEntropyLoss2d(weight)\n",
        "    print(type(criterion))\n",
        "\n",
        "    savedir = args['savedir']\n",
        "\n",
        "    if (enc):\n",
        "        automated_log_path = savedir + \"/automated_log_encoder.txt\"\n",
        "        modeltxtpath = savedir + \"/model_encoder.txt\"\n",
        "    else:\n",
        "        automated_log_path = savedir + \"/automated_log.txt\"\n",
        "        modeltxtpath = savedir + \"/model.txt\"\n",
        "\n",
        "    if (not os.path.exists(automated_log_path)):    #dont add first line if it exists\n",
        "        with open(automated_log_path, \"a\") as myfile:\n",
        "            myfile.write(\"Epoch\\t\\tTrain-loss\\t\\tTest-loss\\t\\tTrain-IoU\\t\\tTest-IoU\\t\\tlearningRate\")\n",
        "\n",
        "    with open(modeltxtpath, \"w\") as myfile:\n",
        "        myfile.write(str(model))\n",
        "\n",
        "    optimizer = Adam(model.parameters(), 2e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4)\n",
        "\n",
        "    start_epoch = 1\n",
        "    if args['resume']:\n",
        "        #Must load weights, optimizer, epoch and best value.\n",
        "        if enc:\n",
        "            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\n",
        "        else:\n",
        "            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\n",
        "\n",
        "        assert os.path.exists(filenameCheckpoint), \"Error: resume option was used but checkpoint was not found in folder\"\n",
        "        checkpoint = torch.load(filenameCheckpoint)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        print(\"=> Loaded checkpoint at epoch {})\".format(checkpoint['epoch']))\n",
        "\n",
        "    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5) # set up scheduler     ## scheduler 1\n",
        "    lambda1 = lambda epoch: pow((1-((epoch-1)/args['num_epochs'])),0.9)  ## scheduler 2\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)                             ## scheduler 2\n",
        "\n",
        "    if args['visualize'] and args['steps_plot'] > 0:\n",
        "        board = Dashboard(8097)\n",
        "\n",
        "    for epoch in range(start_epoch, args['num_epochs']+1):\n",
        "        print(\"----- TRAINING - EPOCH\", epoch, \"-----\")\n",
        "        print(weight)\n",
        "        scheduler.step(epoch)    ## scheduler 2\n",
        "\n",
        "        epoch_loss = []\n",
        "        time_train = []\n",
        "\n",
        "        doIouTrain = args['iouTrain']\n",
        "        doIouVal =  args['iouVal']\n",
        "\n",
        "        if (doIouTrain):\n",
        "            iouEvalTrain = iouEval(NUM_CLASSES)\n",
        "\n",
        "        usedLr = 0\n",
        "        for param_group in optimizer.param_groups:\n",
        "            print(\"LEARNING RATE: \", param_group['lr'])\n",
        "            usedLr = float(param_group['lr'])\n",
        "\n",
        "        model.train()\n",
        "        for step, (images, labels) in enumerate(loader):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            if args['cuda']:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            inputs = Variable(images)\n",
        "            targets = Variable(labels)\n",
        "            outputs = model(inputs)\n",
        "            #print(outputs.shape)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(outputs, targets[:, 0])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss.append(loss.data.item())\n",
        "            time_train.append(time.time() - start_time)\n",
        "\n",
        "            if (doIouTrain):\n",
        "                iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
        "\n",
        "            #print(outputs.size())\n",
        "            if args['visualize'] and args['steps_plot'] > 0 and step % args['steps_plot'] == 0:\n",
        "                start_time_plot = time.time()\n",
        "                image = inputs[0].cpu().data\n",
        "\n",
        "                board.image(image, f'input (epoch: {epoch}, step: {step})')\n",
        "                if isinstance(outputs, list):   #merge gpu tensors\n",
        "                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'output (epoch: {epoch}, step: {step})')\n",
        "                else:\n",
        "                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'output (epoch: {epoch}, step: {step})')\n",
        "                board.image(color_transform(targets[0].cpu().data),\n",
        "                    f'target (epoch: {epoch}, step: {step})')\n",
        "                print (\"Time to paint images: \", time.time() - start_time_plot)\n",
        "            if args['steps_loss'] > 0 and step % args['steps_loss'] == 0:\n",
        "                average = sum(epoch_loss) / len(epoch_loss)\n",
        "                print(f'loss: {average:0.4} (epoch: {epoch}, step: {step})',\n",
        "                        \"// Avg time/img: %.4f s\" % (sum(time_train) / len(time_train) / args['batch_size']))\n",
        "\n",
        "\n",
        "        average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n",
        "\n",
        "        iouTrain = 0\n",
        "        if (doIouTrain):\n",
        "            iouTrain, iou_classes = iouEvalTrain.getIoU()\n",
        "            iouStr = getColorEntry(iouTrain)+'{:0.2f}'.format(iouTrain*100) + '\\033[0m'\n",
        "            print (\"EPOCH IoU on TRAIN set: \", iouStr, \"%\")\n",
        "\n",
        "        #Validate on 500 val images after each epoch of training\n",
        "        print(\"----- VALIDATING - EPOCH\", epoch, \"-----\")\n",
        "        model.eval()\n",
        "        epoch_loss_val = []\n",
        "        time_val = []\n",
        "\n",
        "        if (doIouVal):\n",
        "            iouEvalVal = iouEval(NUM_CLASSES)\n",
        "\n",
        "        for step, (images, labels) in enumerate(loader_val):\n",
        "            start_time = time.time()\n",
        "            if args['cuda']:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
        "            targets = Variable(labels, volatile=True)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets[:, 0])\n",
        "            epoch_loss_val.append(loss.data.item())\n",
        "            time_val.append(time.time() - start_time)\n",
        "\n",
        "\n",
        "            #Add batch to calculate TP, FP and FN for iou estimation\n",
        "            if (doIouVal):\n",
        "                iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
        "\n",
        "            if args['visualize'] and args['steps_plot'] > 0 and step % args['steps_plot'] == 0:\n",
        "                start_time_plot = time.time()\n",
        "                image = inputs[0].cpu().data\n",
        "                board.image(image, f'VAL input (epoch: {epoch}, step: {step})')\n",
        "                if isinstance(outputs, list):   #merge gpu tensors\n",
        "                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'VAL output (epoch: {epoch}, step: {step})')\n",
        "                else:\n",
        "                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n",
        "                    f'VAL output (epoch: {epoch}, step: {step})')\n",
        "                board.image(color_transform(targets[0].cpu().data),\n",
        "                    f'VAL target (epoch: {epoch}, step: {step})')\n",
        "                print (\"Time to paint images: \", time.time() - start_time_plot)\n",
        "            if args['steps_loss'] > 0 and step % args['steps_loss'] == 0:\n",
        "                average = sum(epoch_loss_val) / len(epoch_loss_val)\n",
        "                print(f'VAL loss: {average:0.4} (epoch: {epoch}, step: {step})',\n",
        "                        \"// Avg time/img: %.4f s\" % (sum(time_val) / len(time_val) / args['batch_size']))\n",
        "\n",
        "\n",
        "        average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)\n",
        "\n",
        "        iouVal = 0\n",
        "        if (doIouVal):\n",
        "            iouVal, iou_classes = iouEvalVal.getIoU()\n",
        "            iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\n",
        "            print (\"EPOCH IoU on VAL set: \", iouStr, \"%\")\n",
        "\n",
        "\n",
        "        if iouVal == 0:\n",
        "            current_acc = -average_epoch_loss_val\n",
        "        else:\n",
        "            current_acc = iouVal\n",
        "        is_best = current_acc > best_acc\n",
        "        best_acc = max(current_acc, best_acc)\n",
        "        if enc:\n",
        "            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\n",
        "            filenameBest = savedir + '/model_best_enc.pth.tar'\n",
        "        else:\n",
        "            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\n",
        "            filenameBest = savedir + '/model_best.pth.tar'\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'arch': str(model),\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_acc': best_acc,\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "        }, is_best, filenameCheckpoint, filenameBest)\n",
        "\n",
        "        #SAVE MODEL AFTER EPOCH\n",
        "        if (enc):\n",
        "            filename = f'{savedir}/model_encoder-{epoch:03}.pth'\n",
        "            filenamebest = f'{savedir}/model_encoder_best.pth'\n",
        "        else:\n",
        "            filename = f'{savedir}/model-{epoch:03}.pth'\n",
        "            filenamebest = f'{savedir}/model_best.pth'\n",
        "        if args['epochs_save'] > 0 and step > 0 and step % args['epochs_save'] == 0:\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            print(f'save: {filename} (epoch: {epoch})')\n",
        "        if (is_best):\n",
        "            torch.save(model.state_dict(), filenamebest)\n",
        "            print(f'save: {filenamebest} (epoch: {epoch})')\n",
        "            if (not enc):\n",
        "                with open(savedir + \"/best.txt\", \"w\") as myfile:\n",
        "                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))\n",
        "            else:\n",
        "                with open(savedir + \"/best_encoder.txt\", \"w\") as myfile:\n",
        "                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))\n",
        "\n",
        "        #SAVE TO FILE A ROW WITH THE EPOCH RESULT (train loss, val loss, train IoU, val IoU)\n",
        "        #Epoch\t\tTrain-loss\t\tTest-loss\tTrain-IoU\tTest-IoU\t\tlearningRate\n",
        "        with open(automated_log_path, \"a\") as myfile:\n",
        "            myfile.write(\"\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.8f\" % (epoch, average_epoch_loss_train, average_epoch_loss_val, iouTrain, iouVal, usedLr ))\n",
        "\n",
        "    return(model)   #return model (convenience for encoder-decoder training)\n",
        "\n",
        "def save_checkpoint(state, is_best, filenameCheckpoint, filenameBest):\n",
        "    torch.save(state, filenameCheckpoint)\n",
        "    if is_best:\n",
        "        print (\"Saving model as best\")\n",
        "        torch.save(state, filenameBest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cBg2HxIM-x3",
        "outputId": "466c1424-60fb-415a-df5b-0b60b8775385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AnomalySegmentation_AML/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call training"
      ],
      "metadata": {
        "id": "tPGXdP4zNUFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import logging.handlers\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "savedir = f'../save/Erfnet_finetuned_20_classes'\n",
        "if not os.path.exists(savedir):\n",
        "        os.makedirs(savedir)\n",
        "\n",
        "args = {\n",
        "    'height': 512,\n",
        "    'num_workers': 0,\n",
        "    'batch_size' : 4,\n",
        "    'cuda' : True,\n",
        "    'savedir' : savedir,\n",
        "    'resume': False,\n",
        "    'num_epochs': 5,\n",
        "    'steps_plot' : 50,\n",
        "    'visualize' : False,\n",
        "    'iouTrain': False,\n",
        "    'iouVal': True,\n",
        "    'steps_loss' : 50,\n",
        "    'epochs_save': 0,\n",
        "    'datadir': f'/content/Datasets/Cityscapes/',\n",
        "    'resume': False\n",
        "}\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "   splits = name.split('.')\n",
        "   if str(splits[0]) == 'encoder':\n",
        "      param.requires_grad = False\n",
        "   else:\n",
        "      param.requires_grad = True\n",
        "\n",
        "erfnet_finetuned_20_classes = train(args, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vwLnHNEzNUzJ",
        "outputId": "5fc15d81-62b3-4b18-a080-1b6196dea4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.initial_block.conv.weight\n",
            "encoder.initial_block.conv.bias\n",
            "encoder.initial_block.bn.weight\n",
            "encoder.initial_block.bn.bias\n",
            "encoder.layers.0.conv.weight\n",
            "encoder.layers.0.conv.bias\n",
            "encoder.layers.0.bn.weight\n",
            "encoder.layers.0.bn.bias\n",
            "encoder.layers.1.conv3x1_1.weight\n",
            "encoder.layers.1.conv3x1_1.bias\n",
            "encoder.layers.1.conv1x3_1.weight\n",
            "encoder.layers.1.conv1x3_1.bias\n",
            "encoder.layers.1.bn1.weight\n",
            "encoder.layers.1.bn1.bias\n",
            "encoder.layers.1.conv3x1_2.weight\n",
            "encoder.layers.1.conv3x1_2.bias\n",
            "encoder.layers.1.conv1x3_2.weight\n",
            "encoder.layers.1.conv1x3_2.bias\n",
            "encoder.layers.1.bn2.weight\n",
            "encoder.layers.1.bn2.bias\n",
            "encoder.layers.2.conv3x1_1.weight\n",
            "encoder.layers.2.conv3x1_1.bias\n",
            "encoder.layers.2.conv1x3_1.weight\n",
            "encoder.layers.2.conv1x3_1.bias\n",
            "encoder.layers.2.bn1.weight\n",
            "encoder.layers.2.bn1.bias\n",
            "encoder.layers.2.conv3x1_2.weight\n",
            "encoder.layers.2.conv3x1_2.bias\n",
            "encoder.layers.2.conv1x3_2.weight\n",
            "encoder.layers.2.conv1x3_2.bias\n",
            "encoder.layers.2.bn2.weight\n",
            "encoder.layers.2.bn2.bias\n",
            "encoder.layers.3.conv3x1_1.weight\n",
            "encoder.layers.3.conv3x1_1.bias\n",
            "encoder.layers.3.conv1x3_1.weight\n",
            "encoder.layers.3.conv1x3_1.bias\n",
            "encoder.layers.3.bn1.weight\n",
            "encoder.layers.3.bn1.bias\n",
            "encoder.layers.3.conv3x1_2.weight\n",
            "encoder.layers.3.conv3x1_2.bias\n",
            "encoder.layers.3.conv1x3_2.weight\n",
            "encoder.layers.3.conv1x3_2.bias\n",
            "encoder.layers.3.bn2.weight\n",
            "encoder.layers.3.bn2.bias\n",
            "encoder.layers.4.conv3x1_1.weight\n",
            "encoder.layers.4.conv3x1_1.bias\n",
            "encoder.layers.4.conv1x3_1.weight\n",
            "encoder.layers.4.conv1x3_1.bias\n",
            "encoder.layers.4.bn1.weight\n",
            "encoder.layers.4.bn1.bias\n",
            "encoder.layers.4.conv3x1_2.weight\n",
            "encoder.layers.4.conv3x1_2.bias\n",
            "encoder.layers.4.conv1x3_2.weight\n",
            "encoder.layers.4.conv1x3_2.bias\n",
            "encoder.layers.4.bn2.weight\n",
            "encoder.layers.4.bn2.bias\n",
            "encoder.layers.5.conv3x1_1.weight\n",
            "encoder.layers.5.conv3x1_1.bias\n",
            "encoder.layers.5.conv1x3_1.weight\n",
            "encoder.layers.5.conv1x3_1.bias\n",
            "encoder.layers.5.bn1.weight\n",
            "encoder.layers.5.bn1.bias\n",
            "encoder.layers.5.conv3x1_2.weight\n",
            "encoder.layers.5.conv3x1_2.bias\n",
            "encoder.layers.5.conv1x3_2.weight\n",
            "encoder.layers.5.conv1x3_2.bias\n",
            "encoder.layers.5.bn2.weight\n",
            "encoder.layers.5.bn2.bias\n",
            "encoder.layers.6.conv.weight\n",
            "encoder.layers.6.conv.bias\n",
            "encoder.layers.6.bn.weight\n",
            "encoder.layers.6.bn.bias\n",
            "encoder.layers.7.conv3x1_1.weight\n",
            "encoder.layers.7.conv3x1_1.bias\n",
            "encoder.layers.7.conv1x3_1.weight\n",
            "encoder.layers.7.conv1x3_1.bias\n",
            "encoder.layers.7.bn1.weight\n",
            "encoder.layers.7.bn1.bias\n",
            "encoder.layers.7.conv3x1_2.weight\n",
            "encoder.layers.7.conv3x1_2.bias\n",
            "encoder.layers.7.conv1x3_2.weight\n",
            "encoder.layers.7.conv1x3_2.bias\n",
            "encoder.layers.7.bn2.weight\n",
            "encoder.layers.7.bn2.bias\n",
            "encoder.layers.8.conv3x1_1.weight\n",
            "encoder.layers.8.conv3x1_1.bias\n",
            "encoder.layers.8.conv1x3_1.weight\n",
            "encoder.layers.8.conv1x3_1.bias\n",
            "encoder.layers.8.bn1.weight\n",
            "encoder.layers.8.bn1.bias\n",
            "encoder.layers.8.conv3x1_2.weight\n",
            "encoder.layers.8.conv3x1_2.bias\n",
            "encoder.layers.8.conv1x3_2.weight\n",
            "encoder.layers.8.conv1x3_2.bias\n",
            "encoder.layers.8.bn2.weight\n",
            "encoder.layers.8.bn2.bias\n",
            "encoder.layers.9.conv3x1_1.weight\n",
            "encoder.layers.9.conv3x1_1.bias\n",
            "encoder.layers.9.conv1x3_1.weight\n",
            "encoder.layers.9.conv1x3_1.bias\n",
            "encoder.layers.9.bn1.weight\n",
            "encoder.layers.9.bn1.bias\n",
            "encoder.layers.9.conv3x1_2.weight\n",
            "encoder.layers.9.conv3x1_2.bias\n",
            "encoder.layers.9.conv1x3_2.weight\n",
            "encoder.layers.9.conv1x3_2.bias\n",
            "encoder.layers.9.bn2.weight\n",
            "encoder.layers.9.bn2.bias\n",
            "encoder.layers.10.conv3x1_1.weight\n",
            "encoder.layers.10.conv3x1_1.bias\n",
            "encoder.layers.10.conv1x3_1.weight\n",
            "encoder.layers.10.conv1x3_1.bias\n",
            "encoder.layers.10.bn1.weight\n",
            "encoder.layers.10.bn1.bias\n",
            "encoder.layers.10.conv3x1_2.weight\n",
            "encoder.layers.10.conv3x1_2.bias\n",
            "encoder.layers.10.conv1x3_2.weight\n",
            "encoder.layers.10.conv1x3_2.bias\n",
            "encoder.layers.10.bn2.weight\n",
            "encoder.layers.10.bn2.bias\n",
            "encoder.layers.11.conv3x1_1.weight\n",
            "encoder.layers.11.conv3x1_1.bias\n",
            "encoder.layers.11.conv1x3_1.weight\n",
            "encoder.layers.11.conv1x3_1.bias\n",
            "encoder.layers.11.bn1.weight\n",
            "encoder.layers.11.bn1.bias\n",
            "encoder.layers.11.conv3x1_2.weight\n",
            "encoder.layers.11.conv3x1_2.bias\n",
            "encoder.layers.11.conv1x3_2.weight\n",
            "encoder.layers.11.conv1x3_2.bias\n",
            "encoder.layers.11.bn2.weight\n",
            "encoder.layers.11.bn2.bias\n",
            "encoder.layers.12.conv3x1_1.weight\n",
            "encoder.layers.12.conv3x1_1.bias\n",
            "encoder.layers.12.conv1x3_1.weight\n",
            "encoder.layers.12.conv1x3_1.bias\n",
            "encoder.layers.12.bn1.weight\n",
            "encoder.layers.12.bn1.bias\n",
            "encoder.layers.12.conv3x1_2.weight\n",
            "encoder.layers.12.conv3x1_2.bias\n",
            "encoder.layers.12.conv1x3_2.weight\n",
            "encoder.layers.12.conv1x3_2.bias\n",
            "encoder.layers.12.bn2.weight\n",
            "encoder.layers.12.bn2.bias\n",
            "encoder.layers.13.conv3x1_1.weight\n",
            "encoder.layers.13.conv3x1_1.bias\n",
            "encoder.layers.13.conv1x3_1.weight\n",
            "encoder.layers.13.conv1x3_1.bias\n",
            "encoder.layers.13.bn1.weight\n",
            "encoder.layers.13.bn1.bias\n",
            "encoder.layers.13.conv3x1_2.weight\n",
            "encoder.layers.13.conv3x1_2.bias\n",
            "encoder.layers.13.conv1x3_2.weight\n",
            "encoder.layers.13.conv1x3_2.bias\n",
            "encoder.layers.13.bn2.weight\n",
            "encoder.layers.13.bn2.bias\n",
            "encoder.layers.14.conv3x1_1.weight\n",
            "encoder.layers.14.conv3x1_1.bias\n",
            "encoder.layers.14.conv1x3_1.weight\n",
            "encoder.layers.14.conv1x3_1.bias\n",
            "encoder.layers.14.bn1.weight\n",
            "encoder.layers.14.bn1.bias\n",
            "encoder.layers.14.conv3x1_2.weight\n",
            "encoder.layers.14.conv3x1_2.bias\n",
            "encoder.layers.14.conv1x3_2.weight\n",
            "encoder.layers.14.conv1x3_2.bias\n",
            "encoder.layers.14.bn2.weight\n",
            "encoder.layers.14.bn2.bias\n",
            "encoder.output_conv.weight\n",
            "encoder.output_conv.bias\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nstate_dict = model.state_dict()\\nfor key, value in state_dict.items():\\n  if 'encoder' in key:\\n    #value.autorequires_grad(False)\\n    print((keu))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nklheYqbRY1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}